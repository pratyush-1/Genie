{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFalcon\n",
    "## Common Task 2. Jets as graphs\n",
    "\n",
    "* Please choose a graph-based GNN model of your choice to classify (quark/gluon) jets. Proceed as follows:\n",
    "    1. Convert the images into a point cloud dataset by only considering the non-zero pixels for every event.\n",
    "    2. Cast the point cloud data into a graph representation by coming up with suitable representations for nodes and edges.\n",
    "    3. Train your model on the obtained graph representations of the jet events.\n",
    "* Discuss the resulting performance of the chosen architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genie\n",
    "## Common Task 2. Jets as graphs\n",
    "\n",
    "* Please choose a graph-based GNN model of your choice to classify (quark/gluon) jets. Proceed as follows:\n",
    "    1. Convert the images into a point cloud dataset by only considering the non-zero pixels for every event.\n",
    "    2. Cast the point cloud data into a graph representation by coming up with suitable representations for nodes and edges.\n",
    "    3. Train your model on the obtained graph representations of the jet events.\n",
    "* Discuss the resulting performance of the chosen architecture. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44548/2131603270.py:8: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import h5py\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torchvision\n",
    "import random\n",
    "import cv2\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv,global_mean_pool,GATConv,SAGEConv,GraphConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.neighbors import kneighbors_graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEEDING -  For Reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'quark-gluon_data-set_n139306.hdf5'\n",
    "num_samples = 15000\n",
    "\n",
    "x_jets = np.array(h5py.File(data_path,'r')['X_jets'][:num_samples])\n",
    "labels  = np.array(h5py.File(data_path,'r')['y'][:num_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 125, 125, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_jets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_cloud(x_jets):\n",
    "    point_clouds = []\n",
    "    for img in x_jets:\n",
    "        nonzero_coords = np.any(img!=[0,0,0],axis=-1)\n",
    "        values = img[nonzero_coords]\n",
    "        point_clouds.append(values)\n",
    "    return point_clouds\n",
    "\n",
    "def graph_representation(point_clouds,n_neighbor = 10):\n",
    "    graph_representation = []\n",
    "    for i,point_cloud in enumerate(point_clouds):\n",
    "        edges = kneighbors_graph(point_cloud,n_neighbors=n_neighbor,mode='connectivity')\n",
    "\n",
    "        edges = edges.tocoo()\n",
    "\n",
    "        edge_index = torch.tensor(np.vstack((edges.row,edges.col))).type(torch.long)\n",
    "        edge_attr = torch.tensor(edges.data.reshape(-1,1))\n",
    "        label  = torch.tensor(int(labels[i]),dtype=torch.long)\n",
    "        data = torch_geometric.data.Data(x = torch.tensor(point_cloud), edge_index = edge_index, edge_attr=edge_attr,y=label)\n",
    "        graph_representation.append(data)\n",
    "\n",
    "    return graph_representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_clouds = point_cloud(x_jets)\n",
    "dataset = graph_representation(point_clouds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:10000]\n",
    "val_dataset = dataset[10000:12000]\n",
    "test_dataset = dataset[12000:]\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[330, 3], edge_index=[2, 3300], edge_attr=[3300, 1], y=[1])\n",
      "=============================================================\n",
      "number of feature: 3\n",
      "Number of nodes: 330\n",
      "Number of edges: 3300\n",
      "Average node degree: 10.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'number of feature: {data.num_features}')\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for step,data in enumerate(train_dataloader):\n",
    "#     print(f'Step {step + 1}:')\n",
    "#     print('=======')\n",
    "#     print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "#     print(data)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ) Graph Convolutional Networks (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,in_channels ,hidden_dim, output_dim,p=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels,hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim,hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim,hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim,hidden_dim*2)\n",
    "        self.fc2= nn.Linear(hidden_dim*2,output_dim)\n",
    "        self.p = p\n",
    "    def forward(self,data):\n",
    "        x, edge_index, edge_attr,batch = data.x.float(), data.edge_index, data.edge_attr.float(),data.batch\n",
    "        x = self.conv1(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.conv2(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.conv3(x,edge_index)\n",
    "\n",
    "        x = global_mean_pool(x,batch) #[batch_size,hidden_channels]\n",
    "        \n",
    "        x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN(in_channels=3,hidden_dim=128,output_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.6863 Val Loss: 0.6412, Val Acc: 0.6770\n",
      "Epoch: 2, Train Loss: 0.5163 Val Loss: 0.6225, Val Acc: 0.6975\n",
      "Epoch: 3, Train Loss: 0.7050 Val Loss: 0.6167, Val Acc: 0.6965\n",
      "Epoch: 4, Train Loss: 0.5615 Val Loss: 0.6164, Val Acc: 0.6880\n",
      "Epoch: 5, Train Loss: 0.5614 Val Loss: 0.6077, Val Acc: 0.6995\n",
      "Epoch: 6, Train Loss: 0.4354 Val Loss: 0.6053, Val Acc: 0.7010\n",
      "Epoch: 7, Train Loss: 0.5993 Val Loss: 0.6023, Val Acc: 0.6990\n",
      "Epoch: 8, Train Loss: 0.5784 Val Loss: 0.6017, Val Acc: 0.7010\n",
      "Epoch: 9, Train Loss: 0.6372 Val Loss: 0.5990, Val Acc: 0.7025\n",
      "Epoch: 10, Train Loss: 0.6017 Val Loss: 0.5964, Val Acc: 0.6990\n",
      "Epoch: 11, Train Loss: 0.5384 Val Loss: 0.5995, Val Acc: 0.6955\n",
      "Epoch: 12, Train Loss: 0.5065 Val Loss: 0.5956, Val Acc: 0.6970\n",
      "Epoch: 13, Train Loss: 0.5417 Val Loss: 0.5929, Val Acc: 0.6990\n",
      "Epoch: 14, Train Loss: 0.4236 Val Loss: 0.5964, Val Acc: 0.6980\n",
      "Epoch: 15, Train Loss: 0.4347 Val Loss: 0.6050, Val Acc: 0.6825\n",
      "Epoch: 16, Train Loss: 0.7658 Val Loss: 0.5942, Val Acc: 0.6985\n",
      "Epoch: 17, Train Loss: 0.9048 Val Loss: 0.5902, Val Acc: 0.6945\n",
      "Epoch: 18, Train Loss: 0.6492 Val Loss: 0.5905, Val Acc: 0.6945\n",
      "Epoch: 19, Train Loss: 0.5139 Val Loss: 0.5996, Val Acc: 0.6845\n",
      "Epoch: 20, Train Loss: 0.8060 Val Loss: 0.5904, Val Acc: 0.6980\n",
      "Epoch: 21, Train Loss: 0.4595 Val Loss: 0.5888, Val Acc: 0.6970\n",
      "Epoch: 22, Train Loss: 0.6489 Val Loss: 0.5904, Val Acc: 0.6970\n",
      "Epoch: 23, Train Loss: 0.6757 Val Loss: 0.5975, Val Acc: 0.6920\n",
      "Epoch: 24, Train Loss: 0.5330 Val Loss: 0.5962, Val Acc: 0.6850\n",
      "Epoch: 25, Train Loss: 0.5630 Val Loss: 0.5886, Val Acc: 0.6970\n",
      "Epoch: 26, Train Loss: 0.5413 Val Loss: 0.5885, Val Acc: 0.7005\n",
      "Epoch: 27, Train Loss: 0.6915 Val Loss: 0.5877, Val Acc: 0.6975\n",
      "Epoch: 28, Train Loss: 0.7688 Val Loss: 0.5910, Val Acc: 0.6950\n",
      "Epoch: 29, Train Loss: 0.3441 Val Loss: 0.5897, Val Acc: 0.6970\n",
      "Epoch: 30, Train Loss: 0.6735 Val Loss: 0.5887, Val Acc: 0.6950\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    for train_data in train_dataloader:\n",
    "        train_data = train_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        train_output = model(train_data)\n",
    "        loss = criterion(train_output, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    for val_data in val_dataloader:\n",
    "        val_data = val_data.to(device)\n",
    "        val_output = model(val_data)\n",
    "        val_loss += criterion(val_output, val_data.y).item()\n",
    "        val_pred = val_output.argmax(dim=1)\n",
    "        # print(len(pred))\n",
    "        val_correct += int((val_pred==val_data.y).sum())\n",
    "        # val_correct += (pred == val_data.y).sum().float()/pred.shape[0]\n",
    "   \n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    # print(f'val_correct{val_correct}')\n",
    "    val_acc = val_correct / len(val_dataloader.dataset)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {loss:.4f} Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6947\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "for data in test_dataloader:\n",
    "    data = data.to(device)\n",
    "    output = model(data)\n",
    "    pred = output.argmax(dim=1)\n",
    "    test_correct += int((pred==data.y).sum())\n",
    "\n",
    "test_acc = test_correct / len(test_dataloader.dataset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_accuracy  = 0.6887, valid_accuracy = 0.6865 for n_neigbors = 2\n",
    "#test_accuracy  = 0.6943, valid_accuracy = 0.6885 for n_neigbors = 5\n",
    "#test_accuracy  = 0.6947, valid_accuracy = 0.6950 for n_neigbors = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Graph Attention Networks (GAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self,in_channels ,hidden_dim, output_dim,p=0.3):\n",
    "        super().__init__()\n",
    "        self.attn1 = GATConv(in_channels,hidden_dim)\n",
    "        self.attn2 = GATConv(hidden_dim,hidden_dim)\n",
    "        self.attn3 = GATConv(hidden_dim,hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.fc2= nn.Linear(hidden_dim,output_dim)\n",
    "        self.p = p\n",
    "    def forward(self,data):\n",
    "        x, edge_index, edge_attr,batch = data.x.float(), data.edge_index, data.edge_attr.float(),data.batch\n",
    "        x = self.attn1(x,edge_index, edge_attr = edge_attr)\n",
    "        x = F.relu(x)\n",
    "        # x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.attn2(x,edge_index,edge_attr = edge_attr)\n",
    "        x = F.relu(x)\n",
    "        # x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.attn3(x,edge_index,edge_attr = edge_attr)\n",
    "\n",
    "        x = global_mean_pool(x,batch) \n",
    "        \n",
    "        x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GAT(in_channels=3,hidden_dim=128,output_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.4588 Val Loss: 0.6330, Val Acc: 0.6980\n",
      "Epoch: 2, Train Loss: 0.4638 Val Loss: 0.6370, Val Acc: 0.6985\n",
      "Epoch: 3, Train Loss: 0.5474 Val Loss: 0.6122, Val Acc: 0.6865\n",
      "Epoch: 4, Train Loss: 0.6831 Val Loss: 0.6140, Val Acc: 0.6970\n",
      "Epoch: 5, Train Loss: 0.6498 Val Loss: 0.6037, Val Acc: 0.6920\n",
      "Epoch: 6, Train Loss: 0.7989 Val Loss: 0.6030, Val Acc: 0.6940\n",
      "Epoch: 7, Train Loss: 0.5855 Val Loss: 0.6046, Val Acc: 0.7020\n",
      "Epoch: 8, Train Loss: 0.6341 Val Loss: 0.5980, Val Acc: 0.6930\n",
      "Epoch: 9, Train Loss: 0.6395 Val Loss: 0.5951, Val Acc: 0.7020\n",
      "Epoch: 10, Train Loss: 0.5261 Val Loss: 0.5917, Val Acc: 0.7035\n",
      "Epoch: 11, Train Loss: 0.5162 Val Loss: 0.5881, Val Acc: 0.7010\n",
      "Epoch: 12, Train Loss: 0.6346 Val Loss: 0.5878, Val Acc: 0.6955\n",
      "Epoch: 13, Train Loss: 0.6418 Val Loss: 0.5955, Val Acc: 0.6920\n",
      "Epoch: 14, Train Loss: 0.6222 Val Loss: 0.5826, Val Acc: 0.6995\n",
      "Epoch: 15, Train Loss: 0.7504 Val Loss: 0.5998, Val Acc: 0.6905\n",
      "Epoch: 16, Train Loss: 0.5288 Val Loss: 0.5859, Val Acc: 0.6980\n",
      "Epoch: 17, Train Loss: 0.4892 Val Loss: 0.5970, Val Acc: 0.6900\n",
      "Epoch: 18, Train Loss: 0.6313 Val Loss: 0.5867, Val Acc: 0.7015\n",
      "Epoch: 19, Train Loss: 0.4507 Val Loss: 0.5844, Val Acc: 0.6990\n",
      "Epoch: 20, Train Loss: 0.7316 Val Loss: 0.5834, Val Acc: 0.6940\n",
      "Epoch: 21, Train Loss: 0.4980 Val Loss: 0.5895, Val Acc: 0.6970\n",
      "Epoch: 22, Train Loss: 0.7068 Val Loss: 0.5811, Val Acc: 0.6970\n",
      "Epoch: 23, Train Loss: 0.6937 Val Loss: 0.5856, Val Acc: 0.6975\n",
      "Epoch: 24, Train Loss: 0.5826 Val Loss: 0.6015, Val Acc: 0.6965\n",
      "Epoch: 25, Train Loss: 0.5293 Val Loss: 0.5853, Val Acc: 0.6990\n",
      "Epoch: 26, Train Loss: 0.3775 Val Loss: 0.5837, Val Acc: 0.7040\n",
      "Epoch: 27, Train Loss: 0.7140 Val Loss: 0.5850, Val Acc: 0.6970\n",
      "Epoch: 28, Train Loss: 0.7173 Val Loss: 0.5843, Val Acc: 0.6985\n",
      "Epoch: 29, Train Loss: 0.8186 Val Loss: 0.5845, Val Acc: 0.7005\n",
      "Epoch: 30, Train Loss: 0.7919 Val Loss: 0.5817, Val Acc: 0.6995\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model.train()\n",
    "\n",
    "    for train_data in train_dataloader:\n",
    "        train_data = train_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        train_output = model(train_data)\n",
    "        loss = criterion(train_output, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    for val_data in val_dataloader:\n",
    "        val_data = val_data.to(device)\n",
    "        val_output = model(val_data)\n",
    "        val_loss += criterion(val_output, val_data.y).item()\n",
    "        pred = val_output.argmax(dim=1)\n",
    "        # print(len(pred))\n",
    "        val_correct += int((pred==val_data.y).sum())\n",
    "        # val_correct += (pred == val_data.y).sum().float()/pred.shape[0]\n",
    "   \n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    # print(f'val_correct{val_correct}')\n",
    "    val_acc = val_correct / len(val_dataloader.dataset)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {loss:.4f} Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6917\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "for data in test_dataloader:\n",
    "    data = data.to(device)\n",
    "    output = model(data)\n",
    "    pred = output.argmax(dim=1)\n",
    "    test_correct += int((pred==data.y).sum())\n",
    "\n",
    "test_acc = test_correct / len(test_dataloader.dataset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_accuracy  = 0.6870,valid_accuracy = 0.6955 for n_neigbors = 2\n",
    "# test_accuracy  = 0.6917,valid_accuracy = 0.6980 for n_neigbors = 5\n",
    "# test_accuracy  = 0.6950,valid_accuracy = 0.7060 for n_neigbors = 10\n",
    "\n",
    "# test_accuracy  = 0.6917,valid_accuracy = 0.6995 for n_neigbors = 10 with edge_attr present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) SageConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage(nn.Module):\n",
    "    def __init__(self,in_channels ,hidden_dim, output_dim,p=0.3):\n",
    "        super().__init__()\n",
    "        self.attn1 = SAGEConv(in_channels,hidden_dim)\n",
    "        self.attn2 = SAGEConv(hidden_dim,hidden_dim)\n",
    "        self.attn3 = SAGEConv(hidden_dim,hidden_dim)\n",
    "        self.fc1= nn.Linear(hidden_dim,hidden_dim*2)\n",
    "        self.fc2= nn.Linear(hidden_dim*2,output_dim)\n",
    "        self.p = p\n",
    "    def forward(self,data):\n",
    "        x, edge_index, edge_attr,batch = data.x.float(), data.edge_index, data.edge_attr.float(),data.batch\n",
    "        x = self.attn1(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        # x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.attn2(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        # x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.attn3(x,edge_index)\n",
    "\n",
    "        x = global_mean_pool(x,batch)\n",
    "        \n",
    "        x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GraphSage(in_channels=3,hidden_dim=128,output_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.6685 Val Loss: 0.6915, Val Acc: 0.5030\n",
      "Epoch: 2, Train Loss: 0.6329 Val Loss: 0.6222, Val Acc: 0.6990\n",
      "Epoch: 3, Train Loss: 0.4058 Val Loss: 0.6138, Val Acc: 0.6865\n",
      "Epoch: 4, Train Loss: 0.7051 Val Loss: 0.6017, Val Acc: 0.7035\n",
      "Epoch: 5, Train Loss: 0.7029 Val Loss: 0.6215, Val Acc: 0.6725\n",
      "Epoch: 6, Train Loss: 0.5701 Val Loss: 0.5905, Val Acc: 0.7060\n",
      "Epoch: 7, Train Loss: 0.6021 Val Loss: 0.5999, Val Acc: 0.6945\n",
      "Epoch: 8, Train Loss: 0.5139 Val Loss: 0.5835, Val Acc: 0.7120\n",
      "Epoch: 9, Train Loss: 0.6240 Val Loss: 0.5852, Val Acc: 0.7055\n",
      "Epoch: 10, Train Loss: 0.6043 Val Loss: 0.5853, Val Acc: 0.7075\n",
      "Epoch: 11, Train Loss: 0.6594 Val Loss: 0.5897, Val Acc: 0.7090\n",
      "Epoch: 12, Train Loss: 0.5174 Val Loss: 0.5847, Val Acc: 0.7105\n",
      "Epoch: 13, Train Loss: 0.7411 Val Loss: 0.5864, Val Acc: 0.7120\n",
      "Epoch: 14, Train Loss: 0.6995 Val Loss: 0.5915, Val Acc: 0.6980\n",
      "Epoch: 15, Train Loss: 0.3893 Val Loss: 0.5937, Val Acc: 0.7060\n",
      "Epoch: 16, Train Loss: 0.5466 Val Loss: 0.5812, Val Acc: 0.7090\n",
      "Epoch: 17, Train Loss: 0.7335 Val Loss: 0.5847, Val Acc: 0.7075\n",
      "Epoch: 18, Train Loss: 0.4805 Val Loss: 0.5802, Val Acc: 0.7075\n",
      "Epoch: 19, Train Loss: 0.5172 Val Loss: 0.5973, Val Acc: 0.7015\n",
      "Epoch: 20, Train Loss: 0.5840 Val Loss: 0.5900, Val Acc: 0.7050\n",
      "Epoch: 21, Train Loss: 0.4945 Val Loss: 0.5827, Val Acc: 0.7090\n",
      "Epoch: 22, Train Loss: 0.5311 Val Loss: 0.5844, Val Acc: 0.7065\n",
      "Epoch: 23, Train Loss: 0.5105 Val Loss: 0.5804, Val Acc: 0.7075\n",
      "Epoch: 24, Train Loss: 0.5608 Val Loss: 0.5863, Val Acc: 0.6985\n",
      "Epoch: 25, Train Loss: 0.5879 Val Loss: 0.5878, Val Acc: 0.7020\n",
      "Epoch: 26, Train Loss: 0.7116 Val Loss: 0.5834, Val Acc: 0.7115\n",
      "Epoch: 27, Train Loss: 0.6727 Val Loss: 0.5824, Val Acc: 0.7040\n",
      "Epoch: 28, Train Loss: 0.4441 Val Loss: 0.5821, Val Acc: 0.7080\n",
      "Epoch: 29, Train Loss: 0.5386 Val Loss: 0.5831, Val Acc: 0.7055\n",
      "Epoch: 30, Train Loss: 0.5565 Val Loss: 0.5813, Val Acc: 0.7080\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    \n",
    "    for train_data in train_dataloader:\n",
    "        train_data = train_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        train_output = model(train_data)\n",
    "        loss = criterion(train_output, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    for val_data in val_dataloader:\n",
    "        val_data = val_data.to(device)\n",
    "        val_output = model(val_data)\n",
    "        val_loss += criterion(val_output, val_data.y).item()\n",
    "        pred = val_output.argmax(dim=1)\n",
    "        # print(len(pred))\n",
    "        val_correct += int((pred==val_data.y).sum())\n",
    "        # val_correct += (pred == val_data.y).sum().float()/pred.shape[0]\n",
    "   \n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    # print(f'val_correct{val_correct}')\n",
    "    val_acc = val_correct / len(val_dataloader.dataset)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {loss:.4f} Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6990\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "for data in test_dataloader:\n",
    "    data = data.to(device)\n",
    "    output = model(data)\n",
    "    pred = output.argmax(dim=1)\n",
    "    test_correct += int((pred==data.y).sum())\n",
    "\n",
    "test_acc = test_correct / len(test_dataloader.dataset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_accuracy = 0.6930, valid_accuracy = 0.7040 for n_neighbors =2\n",
    "#test_accuracy = 0.6973, valid_accuracy = 0.7025 for n_neighbors =5\n",
    "#test_accuracy = 0.6990, valid_accuracy = 0.7080 for n_neighbors =10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self,in_channels ,hidden_dim, output_dim,p=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_channels,hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim,hidden_dim)\n",
    "        self.conv3 = GraphConv(hidden_dim,hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim,hidden_dim*2)\n",
    "        self.fc2= nn.Linear(hidden_dim*2,output_dim)\n",
    "        self.p = p\n",
    "    def forward(self,data):\n",
    "        x, edge_index, edge_attr,batch = data.x.float(), data.edge_index, data.edge_attr.float(),data.batch\n",
    "        x = self.conv1(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        # x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.conv2(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        # x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.conv3(x,edge_index)\n",
    "\n",
    "        x = global_mean_pool(x,batch) #[batch_size,hidden_channels]\n",
    "        \n",
    "        x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,p=self.p,training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GNN(in_channels=3,hidden_dim=128,output_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.5948 Val Loss: 0.6839, Val Acc: 0.5365\n",
      "Epoch: 2, Train Loss: 0.5838 Val Loss: 0.6429, Val Acc: 0.6260\n",
      "Epoch: 3, Train Loss: 0.5251 Val Loss: 0.5970, Val Acc: 0.6875\n",
      "Epoch: 4, Train Loss: 0.4603 Val Loss: 0.6087, Val Acc: 0.6995\n",
      "Epoch: 5, Train Loss: 0.5617 Val Loss: 0.5943, Val Acc: 0.6935\n",
      "Epoch: 6, Train Loss: 0.6682 Val Loss: 0.6160, Val Acc: 0.6790\n",
      "Epoch: 7, Train Loss: 0.7295 Val Loss: 0.5928, Val Acc: 0.6945\n",
      "Epoch: 8, Train Loss: 0.6059 Val Loss: 0.6012, Val Acc: 0.7000\n",
      "Epoch: 9, Train Loss: 0.6289 Val Loss: 0.6364, Val Acc: 0.6265\n",
      "Epoch: 10, Train Loss: 0.6390 Val Loss: 0.5893, Val Acc: 0.7045\n",
      "Epoch: 11, Train Loss: 0.6475 Val Loss: 0.5872, Val Acc: 0.7010\n",
      "Epoch: 12, Train Loss: 0.5155 Val Loss: 0.5896, Val Acc: 0.7020\n",
      "Epoch: 13, Train Loss: 0.5945 Val Loss: 0.5890, Val Acc: 0.7035\n",
      "Epoch: 14, Train Loss: 0.5419 Val Loss: 0.5869, Val Acc: 0.7100\n",
      "Epoch: 15, Train Loss: 0.4081 Val Loss: 0.5814, Val Acc: 0.7075\n",
      "Epoch: 16, Train Loss: 0.6927 Val Loss: 0.6162, Val Acc: 0.6885\n",
      "Epoch: 17, Train Loss: 0.6663 Val Loss: 0.5819, Val Acc: 0.7080\n",
      "Epoch: 18, Train Loss: 0.4436 Val Loss: 0.5809, Val Acc: 0.7095\n",
      "Epoch: 19, Train Loss: 0.7219 Val Loss: 0.6027, Val Acc: 0.6890\n",
      "Epoch: 20, Train Loss: 0.4787 Val Loss: 0.5806, Val Acc: 0.7070\n",
      "Epoch: 21, Train Loss: 0.5317 Val Loss: 0.5764, Val Acc: 0.7115\n",
      "Epoch: 22, Train Loss: 0.5787 Val Loss: 0.5891, Val Acc: 0.6975\n",
      "Epoch: 23, Train Loss: 0.5224 Val Loss: 0.5802, Val Acc: 0.7125\n",
      "Epoch: 24, Train Loss: 0.6407 Val Loss: 0.5765, Val Acc: 0.7210\n",
      "Epoch: 25, Train Loss: 0.5306 Val Loss: 0.5816, Val Acc: 0.7105\n",
      "Epoch: 26, Train Loss: 0.3843 Val Loss: 0.5786, Val Acc: 0.7080\n",
      "Epoch: 27, Train Loss: 0.6315 Val Loss: 0.5770, Val Acc: 0.7180\n",
      "Epoch: 28, Train Loss: 0.5156 Val Loss: 0.5897, Val Acc: 0.7010\n",
      "Epoch: 29, Train Loss: 0.4531 Val Loss: 0.5767, Val Acc: 0.7160\n",
      "Epoch: 30, Train Loss: 0.3989 Val Loss: 0.5736, Val Acc: 0.7170\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model.train()\n",
    "\n",
    "    for train_data in train_dataloader:\n",
    "        train_data = train_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        train_output = model(train_data)\n",
    "        loss = criterion(train_output, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    for val_data in val_dataloader:\n",
    "        val_data = val_data.to(device)\n",
    "        val_output = model(val_data)\n",
    "        val_loss += criterion(val_output, val_data.y).item()\n",
    "        pred = val_output.argmax(dim=1)\n",
    "        # print(len(pred))\n",
    "        val_correct += int((pred==val_data.y).sum())\n",
    "        # val_correct += (pred == val_data.y).sum().float()/pred.shape[0]\n",
    "   \n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    # print(f'val_correct{val_correct}')\n",
    "    val_acc = val_correct / len(val_dataloader.dataset)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {loss:.4f} Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7070\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "for data in test_dataloader:\n",
    "    data = data.to(device)\n",
    "    output = model(data)\n",
    "    pred = output.argmax(dim=1)\n",
    "    test_correct += int((pred==data.y).sum())\n",
    "\n",
    "test_acc = test_correct / len(test_dataloader.dataset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_accuracy = 0.6923, valid_accuracy = 0.7090 for n_neighbors =2\n",
    "#test_accuracy = 0.6857, valid_accuracy = 0.6930 for n_neighbors =5\n",
    "#test_accuracy = 0.7070, valid_accuracy = 0.7170 for n_neighbors =10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Test Accuracy | Validation Accuracy | \n",
    "| :-------: | :----: | :----: | \n",
    "| GCN (k=10) | 0.6947 | 0.6950 | \n",
    "| GCN (k=5) | 0.6943 | 0.6885 | \n",
    "| GCN (k=2) |  0.6887 | 0.6865 | \n",
    "| GAT(k=10) | 0.6950 | 0.7060 | \n",
    "| GAT (k=5) | 0.6917 | 0.6980 | \n",
    "| GAT (k=2) |  0.6870 | 0.6955 | \n",
    "| SageConv (k=10) | 0.6990 | 0.7080 | \n",
    "| SageConv (k=5) | 0.6973 | 0.7025 | \n",
    "| SageConv (k=2) |  0.6930 | 0.7040| \n",
    "| GraphConv (k=10) | 0.7070 | 0.7170 | \n",
    "| GraphConv (k=5) | 0.6857 | 0.6930 | \n",
    "| GraphConv (k=2) |  0.6923 | 0.7070 | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISCUSSION\n",
    "\n",
    "* Accuracy difference between the different vaues of n_neighbors is small, indicating that the various architectures employed are not very sensitive and are robust to different values of n_neighbors\n",
    "\n",
    "* Brief about architectures used - \n",
    "    1. GCN operates by aaggregating feature information from neighboring nodes in graph to update central nodes representation. It can only take node features as input.\n",
    "\n",
    "    2. GAT introduces attention mechanisms to weigh importance of neighboring nodes when aggregating information. It can take both node features and edge features.\n",
    "\n",
    "    3. SageConv operates by sampling and aggregating features from neigboring nodes. It incorporates pooling operations to aggregate information from neighboring nodes. It can only take node features as input.\n",
    "\n",
    "    4. GraphConv aggregates information from neighboring nodes using weighted combination of node features.It can only take node features as input.\n",
    "\n",
    "* All of them achieve an accuracy in range of 68-70%, an method to improve the accuracy could be to either deepen the current neural network architectures or apply networks that could model longer range dependencies and capture more complex patterns like Graph Transformer Networks or Graph Isomorphism networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERNCES - \n",
    "\n",
    "1. https://arxiv.org/pdf/2104.01725.pdf\n",
    "2. https://ml4physicalsciences.github.io/2020/files/NeurIPS_ML4PS_2020_138.pdf\n",
    "3. https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
